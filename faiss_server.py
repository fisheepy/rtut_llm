import faiss
import numpy as np
import json
import openai
import os
import glob
from flask import Flask, request, jsonify
from docx import Document
from tqdm import tqdm
from flask_cors import CORS  # ‚úÖ Enable CORS for frontend access
import openai._exceptions 
import boto3

# ‚úÖ Load S3 Credentials from Heroku Environment Variables
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_BUCKET_NAME = os.getenv("AWS_BUCKET_NAME")
AWS_REGION = 'us-east-2'

# ‚úÖ Initialize S3 Client
s3 = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY,
    region_name=AWS_REGION
)

# ‚úÖ Load OpenAI API Key
openai.api_key = os.getenv("OPENAI_API_KEY")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Gets the `/backend/server/llm/` path
INDEX_PATH = os.path.join(BASE_DIR, "faiss_index.bin")
METADATA_PATH = os.path.join(BASE_DIR, "metadata_store.json")

# ‚úÖ Initialize FAISS Index (384 dimensions for MiniLM embeddings)
D = 1536  # ‚úÖ Set FAISS index dimension to match OpenAI embedding size
index = faiss.IndexFlatL2(D)  # L2 distance metric

metadata_store = []  # Stores metadata like policy names

app = Flask(__name__)
CORS(app)

@app.route("/")
def home():
    return "FAISS Server is running."

# ‚úÖ Function to Upload Files to S3
def upload_to_s3(file_path, s3_key):
    try:
        s3.upload_file(file_path, AWS_BUCKET_NAME, s3_key)
        print(f"‚úÖ Uploaded {file_path} to S3 bucket {AWS_BUCKET_NAME}")
    except Exception as e:
        print(f"‚ùå Failed to upload {file_path} to S3: {e}")

# ‚úÖ Function to Download Files from S3
def download_from_s3(s3_key, local_path):
    try:
        s3.download_file(AWS_BUCKET_NAME, s3_key, local_path)
        print(f"‚úÖ Downloaded {s3_key} from S3 to {local_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to download {s3_key} from S3: {e}")

# ‚úÖ Load FAISS and Metadata on Startup
def initialize_faiss():
    global index, metadata_store
    print("üîÑ Initializing FAISS...")

    # Download Index and Metadata from S3
    download_from_s3("faiss_index.bin", INDEX_PATH)
    download_from_s3("metadata_store.json", METADATA_PATH)

    # Load FAISS Index
    if os.path.exists(INDEX_PATH):
        index = faiss.read_index(INDEX_PATH)
        print("‚úÖ FAISS Index Loaded from S3")
    else:
        print("‚ö†Ô∏è No FAISS index found. Run `/train` API to train with documents.")

    # Load Metadata
    if os.path.exists(METADATA_PATH):
        try:
            with open(METADATA_PATH, "r", encoding="utf-8") as f:
                metadata_store = json.load(f)
            print(f"‚úÖ Loaded {len(metadata_store)} policies from metadata store.")
        except json.JSONDecodeError:
            print("‚ùå Failed to load metadata_store.json.")
            metadata_store = []
    else:
        metadata_store = []

# ‚úÖ Call `initialize_faiss()` Immediately (Works for Both Gunicorn & Local)
initialize_faiss()

# ‚úÖ Function to Get Text Embeddings
def get_embedding(text):
    """Get OpenAI embedding for a given text with error handling."""
    try:
        response = openai.embeddings.create(
            model="text-embedding-ada-002",
            input=text
        )
        return response.data[0].embedding
    except openai._exceptions.AuthenticationError:
        print("‚ùå OpenAI API Key is invalid!")
        return None
    except openai._exceptions.RateLimitError:
        print("‚ùå OpenAI Rate Limit exceeded. Try again later.")
        return None
    except openai._exceptions.OpenAIError as e:
        print(f"‚ùå OpenAI Error: {e}")
        return None

# ‚úÖ Function to Extract Text from `.docx`
def extract_text_from_docx(file_path):
    doc = Document(file_path)
    text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
    return text

# ‚úÖ Function to Train FAISS from `.docx` Policies
def train_faiss_from_documents(directory):
    global index, metadata_store
    doc_files = glob.glob(os.path.join(directory, "*.docx"))

    print(f"üìÑ Found {len(doc_files)} policy documents. Processing...")

    all_embeddings = []
    all_metadata = []

    for file_path in tqdm(doc_files, desc="Processing Documents"):
        policy_text = extract_text_from_docx(file_path)
        policy_name = os.path.basename(file_path)

        # ‚úÖ Split into chunks
        chunks = [policy_text[i:i+500] for i in range(0, len(policy_text), 500)]

        for chunk in chunks:
            embedding = get_embedding(chunk)
            all_embeddings.append(embedding)
            all_metadata.append({"name": policy_name, "text": chunk})  # ‚úÖ Store metadata

    if all_embeddings:
        index.add(np.array(all_embeddings, dtype=np.float32))  # ‚úÖ Add all at once
        metadata_store.extend(all_metadata)  # ‚úÖ Ensure metadata is stored!

    # ‚úÖ Save FAISS index
    faiss.write_index(index, INDEX_PATH)

    # ‚úÖ Save metadata to JSON
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata_store, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ FAISS Training Complete! Indexed {len(all_embeddings)} vectors and {len(metadata_store)} policies.")

# ‚úÖ API Endpoint to Train FAISS Automatically
@app.route("/train", methods=["POST"])
def train():
    directory = request.json.get("directory")
    if not directory or not os.path.exists(directory):
        return jsonify({"error": "Invalid directory path"}), 400

    train_faiss_from_documents(directory)
    return jsonify({"message": "Training complete!"})

@app.route("/status", methods=["GET"])
def status():
    return jsonify({
        "num_vectors": index.ntotal,
        "num_policies": len(metadata_store),
        "index_size": index.d,
        "sample_metadata": metadata_store[:5]  # ‚úÖ Print first 5 stored policies
    })

# ‚úÖ API Endpoint to Search FAISS for Relevant Policies
@app.route("/search", methods=["POST"])
def search():
    global index, metadata_store

    data = request.json
    query = data.get("query")

    if not query:
        return jsonify({"error": "Missing search query"}), 400

    # Convert user query to an embedding
    query_embedding = np.array([get_embedding(query)], dtype=np.float32)

    # Retrieve the top 3 most relevant documents
    distances, indices = index.search(query_embedding, k=3)

    results = []
    for idx in indices[0]:
        if idx < len(metadata_store):
            results.append(metadata_store[idx])

    return jsonify({"results": results})

@app.route("/chat", methods=["POST"])
def chat():
    global index, metadata_store

    data = request.json
    question = data.get("question")

    if not question:
        return jsonify({"error": "Missing question"}), 400

    query_embedding = get_embedding(question)
    
    if query_embedding is None:
        return jsonify({"error": "Failed to generate embedding. Check OpenAI API key."}), 500

    # Search FAISS index
    distances, indices = index.search(np.array([query_embedding], dtype=np.float32), k=3)

    results = []
    for idx in indices[0]:
        if idx < len(metadata_store):
            results.append(metadata_store[idx])

    if not results:
        return jsonify({"answer": "No relevant policy found. Please contact HR."})

    context = "\n\n".join([f"**{r['name']}**: {r['text']}" for r in results])

    try:
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": f"Question: {question}\n\nRelevant Policy: {context}"}],
            temperature=0.2
        )

        return jsonify({
            "answer": response.choices[0].message.content,
            "referenced_documents": list(set([r["name"] for r in results]))
        })

    except openai._exceptions.OpenAIError as e:
        print(f"‚ùå OpenAI Error: {e}")
        return jsonify({"error": "AI processing error. Please try again later."}), 500

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5001))  # ‚úÖ Use Heroku's assigned port
    print(f"üöÄ FAISS Server Listening on: http://0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port, debug=True)
